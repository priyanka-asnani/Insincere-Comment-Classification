{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "import operator \n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1306122, 3)\n",
      "(375806, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('Data/quora-insincere-questions-classification/train.csv')\n",
    "test_data = pd.read_csv('Data/quora-insincere-questions-classification/test.csv')\n",
    "\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1306122/1306122 [00:06<00:00, 212657.95it/s]\n",
      "100%|██████████| 1306122/1306122 [00:05<00:00, 231014.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'How': 261930, 'did': 33489, 'Quebec': 97, 'nationalists': 91, 'see': 9003}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(sentences, verbose =  True):\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "sentences = train_data[\"question_text\"].progress_apply(lambda x: x.split()).values\n",
    "vocab = build_vocab(sentences)\n",
    "print({k: vocab[k] for k in list(vocab)[:5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_path = 'Data/quora-insincere-questions-classification/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "embeddings_index = KeyedVectors.load_word2vec_format(news_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 508823/508823 [00:01<00:00, 262272.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 24.31% of vocab\n",
      "Found embeddings for  78.75% of all text\n"
     ]
    }
   ],
   "source": [
    "def check_coverage(vocab,embeddings_index):\n",
    "    a = {}\n",
    "    no_embeddings = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "            no_embeddings[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(no_embeddings.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x\n",
    "\n",
    "no_embed_words = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 403183),\n",
       " ('a', 402682),\n",
       " ('of', 330825),\n",
       " ('and', 251973),\n",
       " ('India?', 16384),\n",
       " ('it?', 12900),\n",
       " ('do?', 8753),\n",
       " ('life?', 7753),\n",
       " ('you?', 6295),\n",
       " ('me?', 6202)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_embed_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1306122/1306122 [00:22<00:00, 58156.27it/s]\n",
      "100%|██████████| 1306122/1306122 [00:05<00:00, 236508.37it/s]\n"
     ]
    }
   ],
   "source": [
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in \"/-\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²--':\n",
    "        x = x.replace(punct, '')\n",
    "    return x\n",
    "\n",
    "train_data[\"question_text\"] = train_data[\"question_text\"].progress_apply(lambda x: clean_text(x))\n",
    "sentences = train_data[\"question_text\"].apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 257503/257503 [00:00<00:00, 268226.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 56.92% of vocab\n",
      "Found embeddings for  89.77% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "no_embed_words = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! We were able to increase our embeddings ratio from 24% to 57% by just handling punctiation. Ok lets check on thos oov words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 406301),\n",
       " ('a', 403830),\n",
       " ('of', 332964),\n",
       " ('and', 254077),\n",
       " ('2017', 8771),\n",
       " ('2018', 7364),\n",
       " ('doesnt', 6786),\n",
       " ('10', 6612),\n",
       " ('didnt', 3886),\n",
       " ('12', 3698),\n",
       " ('100', 2897),\n",
       " ('20', 2885),\n",
       " ('isnt', 2793),\n",
       " ('15', 2766),\n",
       " ('12th', 2551),\n",
       " ('11', 2314),\n",
       " ('30', 2131),\n",
       " ('18', 2046),\n",
       " ('50', 1989),\n",
       " ('16', 1582)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_embed_words[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1306122/1306122 [00:24<00:00, 53157.04it/s]\n",
      "100%|██████████| 1306122/1306122 [00:08<00:00, 155764.54it/s]\n",
      "100%|██████████| 1306122/1306122 [00:05<00:00, 234078.54it/s]\n",
      "100%|██████████| 246996/246996 [00:01<00:00, 244688.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 59.86% of vocab\n",
      "Found embeddings for  90.55% of all text\n"
     ]
    }
   ],
   "source": [
    "## deal with numbers\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('(?<=[0-9][0-9])th', '', x)\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "train_data[\"question_text\"] = train_data[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\n",
    "sentences = train_data[\"question_text\"].progress_apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)\n",
    "\n",
    "no_embed_words = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 406301),\n",
       " ('a', 403830),\n",
       " ('of', 332964),\n",
       " ('and', 254077),\n",
       " ('doesnt', 6786),\n",
       " ('didnt', 3886),\n",
       " ('isnt', 2793),\n",
       " ('Isnt', 1432),\n",
       " ('favourite', 1246),\n",
       " ('bitcoin', 980),\n",
       " ('colour', 976),\n",
       " ('centre', 884),\n",
       " ('Quorans', 879),\n",
       " ('cryptocurrency', 821),\n",
       " ('shouldnt', 797),\n",
       " ('hasnt', 786),\n",
       " ('Snapchat', 785),\n",
       " ('wasnt', 743),\n",
       " ('travelling', 705),\n",
       " ('counselling', 634)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_embed_words[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok now we take care of common misspellings when using american/ british vocab and replacing a few \"modern\" words with \"social media\" for this task I use a multi regex script I found some time ago on stack overflow. Additionally we will simply remove the words \"a\",\"to\",\"and\" and \"of\" since those have obviously been downsampled when training the GoogleNews Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "\n",
    "mispell_dict = {'colour':'color', 'centre':'center', 'didnt':'did not', 'doesnt':'does not', 'isnt':'is not', 'shouldnt':'should not', 'favourite':'favorite',\n",
    "                'travelling':'traveling', 'counselling':'counseling', 'theatre':'theater', 'cancelled':'canceled', 'labour':'labor', 'organisation':'organization',\n",
    "                'wwii':'world war 2', 'citicise':'criticize', 'instagram': 'social medium', 'whatsapp': 'social medium', 'snapchat': 'social medium', \n",
    "                'dont': 'do not', 'Isnt':'is not', 'Doesnt':'does not', 'hasnt':'has not', 'wasnt':'was not', 'behaviour': 'behavior', \n",
    "                'cryptocurrencies': 'crypto currency', 'programme': 'program', 'organisations': 'organization', 'licence': 'license',  'organisation': 'organization',\n",
    "               'Whatis': 'what is', 'favour': 'favor', 'Pinterest': 'social medium', 'learnt': 'learn', 'defence': 'defense', 'recognise': 'recognize',\n",
    "               'recognised': 'recognize', 'practise': 'practice', 'neighbour': 'neighbor', 'programr': 'programmer', 'realise': 'realize', 'Didnt':'did not',\n",
    "               'theatre': 'theater', 'travelling': 'traveling', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', \n",
    "                'demonetisation': 'demonetization', 'narcissit': 'narcissist', 'bigdata': 'big data', 'Qoura': 'Quora', 'sallary': 'salary',\n",
    "               'analyse': 'analyze'}\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1306122/1306122 [00:16<00:00, 79967.69it/s]\n",
      "100%|██████████| 1306122/1306122 [00:06<00:00, 197695.59it/s]\n",
      "100%|██████████| 1306122/1306122 [00:03<00:00, 328521.85it/s]\n",
      "100%|██████████| 1306122/1306122 [00:05<00:00, 245018.53it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data[\"question_text\"] = train_data[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "sentences = train_data[\"question_text\"].progress_apply(lambda x: x.split())\n",
    "to_remove = ['a','to','of','and']\n",
    "sentences = [[word for word in sentence if not word in to_remove] for sentence in tqdm(sentences)]\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 246849/246849 [00:01<00:00, 181430.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 59.88% of vocab\n",
      "Found embeddings for  98.94% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "no_embed_words = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bitcoin', 980),\n",
       " ('Quorans', 880),\n",
       " ('cryptocurrency', 821),\n",
       " ('Snapchat', 785),\n",
       " ('programr', 684),\n",
       " ('btech', 632),\n",
       " ('Brexit', 492),\n",
       " ('Shouldnt', 478),\n",
       " ('blockchain', 474),\n",
       " ('upvotes', 433),\n",
       " ('Redmi', 378),\n",
       " ('KVPY', 349),\n",
       " ('programrs', 337),\n",
       " ('Paytm', 331),\n",
       " ('grey', 299),\n",
       " ('Quoras', 292),\n",
       " ('mtech', 280),\n",
       " ('Btech', 263),\n",
       " ('bitcoins', 261),\n",
       " ('honours', 254)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_embed_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"question_text\"] = train_data[\"question_text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          how did quebec nationalists see their province...\n",
       "1          do you have an adopted dog how would you encou...\n",
       "2          why does velocity affect time does velocity af...\n",
       "3          how did otto von guericke used the magdeburg h...\n",
       "4          can i convert montra helicon d to a mountain b...\n",
       "                                 ...                        \n",
       "1306117    what other technical skills do you need as a c...\n",
       "1306118    does ms in ece have good job prospects in usa ...\n",
       "1306119                             is foam insulation toxic\n",
       "1306120    how can one start a research project based on ...\n",
       "1306121    who wins in a battle between a wolverine and a...\n",
       "Name: question_text, Length: 1306122, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"question_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words(text):\n",
    "    to_remove = ['a','to','of','and']\n",
    "    tokens = text.split(' ')\n",
    "    new_string = ' '.join([token for token in tokens if token not in to_remove])\n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"question_text\"] = train_data[\"question_text\"].apply(remove_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          how did quebec nationalists see their province...\n",
       "1          do you have an adopted dog how would you encou...\n",
       "2          why does velocity affect time does velocity af...\n",
       "3          how did otto von guericke used the magdeburg h...\n",
       "4          can i convert montra helicon d mountain bike b...\n",
       "                                 ...                        \n",
       "1306117    what other technical skills do you need as com...\n",
       "1306118    does ms in ece have good job prospects in usa ...\n",
       "1306119                             is foam insulation toxic\n",
       "1306120    how can one start research project based on bi...\n",
       "1306121            who wins in battle between wolverine puma\n",
       "Name: question_text, Length: 1306122, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"question_text\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1306122/1306122 [00:08<00:00, 150574.65it/s]\n",
      "100%|██████████| 1306122/1306122 [00:06<00:00, 214852.51it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = train_data[\"question_text\"].progress_apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203064"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('preprocessed_data_deep_learning.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
