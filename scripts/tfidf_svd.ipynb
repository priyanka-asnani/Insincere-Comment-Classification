{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1306122, 3)\n"
     ]
    }
   ],
   "source": [
    "## read in the cleaned data\n",
    "df = pd.read_csv('cleaned_quora_data.csv')\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check if the dataframe has any null values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>quebec nationalist see province nation s</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>adopted dog would encourage people adopt shop</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>velocity affect time velocity affect space geo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>otto von guericke used magdeburg hemisphere</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>convert montra helicon mountain bike changing ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6           quebec nationalist see province nation s   \n",
       "1  000032939017120e6e44      adopted dog would encourage people adopt shop   \n",
       "2  0000412ca6e4628ce2cf  velocity affect time velocity affect space geo...   \n",
       "3  000042bf85aa498cd78e        otto von guericke used magdeburg hemisphere   \n",
       "4  0000455dfa3e01eae3af  convert montra helicon mountain bike changing ...   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1305904, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['question_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quebec nationalist see province nation s',\n",
       " 'adopted dog would encourage people adopt shop',\n",
       " 'velocity affect time velocity affect space geometry',\n",
       " 'otto von guericke used magdeburg hemisphere',\n",
       " 'convert montra helicon mountain bike changing tyre',\n",
       " 'gaza slowly becoming auschwitz dachau treblinka palestinian',\n",
       " 'quora automatically ban conservative opinion reported liberal view',\n",
       " 'crazy wash wipe grocery germ everywhere',\n",
       " 'thing dressing moderately different dressing modestly',\n",
       " 'ever phase wherein became ignorant people loved completely disregarding feeling life get something go way feel temporarily ease thing change']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 100)\n",
    "train_text = vectorizer.fit_transform(df['question_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top tf-idf words across all documents\n",
    "weights = np.asarray(train_text.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': vectorizer.get_feature_names(), 'weight': weights})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>best</td>\n",
       "      <td>0.013084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3018</th>\n",
       "      <td>get</td>\n",
       "      <td>0.011954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5242</th>\n",
       "      <td>people</td>\n",
       "      <td>0.010481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7868</th>\n",
       "      <td>would</td>\n",
       "      <td>0.010465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4163</th>\n",
       "      <td>like</td>\n",
       "      <td>0.009783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4235</th>\n",
       "      <td>look</td>\n",
       "      <td>0.002371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7406</th>\n",
       "      <td>type</td>\n",
       "      <td>0.002365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7031</th>\n",
       "      <td>system</td>\n",
       "      <td>0.002351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7703</th>\n",
       "      <td>war</td>\n",
       "      <td>0.002320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3531</th>\n",
       "      <td>important</td>\n",
       "      <td>0.002305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           term    weight\n",
       "761        best  0.013084\n",
       "3018        get  0.011954\n",
       "5242     people  0.010481\n",
       "7868      would  0.010465\n",
       "4163       like  0.009783\n",
       "...         ...       ...\n",
       "4235       look  0.002371\n",
       "7406       type  0.002365\n",
       "7031     system  0.002351\n",
       "7703        war  0.002320\n",
       "3531  important  0.002305\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_df.sort_values(by=[\"weight\"], ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_df.to_csv('weights.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = weights_df['term'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aaa',\n",
       " 'aaaa',\n",
       " 'aaaaa',\n",
       " 'aaabbbcccbaac',\n",
       " 'aaadhar',\n",
       " 'aaak',\n",
       " 'aaaq',\n",
       " 'aaat',\n",
       " 'aab']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf_idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>quebec</th>\n",
       "      <td>0.522078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>province</th>\n",
       "      <td>0.496858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nationalist</th>\n",
       "      <td>0.492266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nation</th>\n",
       "      <td>0.378241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>see</th>\n",
       "      <td>0.308505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gagan</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gaganbawda</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gagaran</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gagarin</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ｈow</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>177187 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               tf_idf\n",
       "quebec       0.522078\n",
       "province     0.496858\n",
       "nationalist  0.492266\n",
       "nation       0.378241\n",
       "see          0.308505\n",
       "...               ...\n",
       "gagan        0.000000\n",
       "gaganbawda   0.000000\n",
       "gagaran      0.000000\n",
       "gagarin      0.000000\n",
       "ｈow          0.000000\n",
       "\n",
       "[177187 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the first vector out (for the first document)\n",
    "first_vector_tfidfvectorizer=train_text[0]\n",
    " \n",
    "# place tf-idf values in a pandas data frame\n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=vectorizer.get_feature_names(), columns=[\"tf_idf\"])\n",
    "df.sort_values(by=[\"tf_idf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## truncating matrix using SVD\n",
    "#n_comp = train_text.shape[1]-1\n",
    "svd = TruncatedSVD(n_components=6074, algorithm='arpack', random_state=42)\n",
    "svd_transform = svd.fit_transform(train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract topics and terms After performing SVD, we need to extract the topics from the component matrix. Let’s see the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the topics with their terms\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for index, component in enumerate(svd.components_):\n",
    "    zipped = zip(terms, component)\n",
    "    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:10]\n",
    "    top_terms_list=list(dict(top_terms_key).keys())\n",
    "    #print(\"Topic \"+str(index)+\": \",top_terms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(svd.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of explained variances\n",
    "svd_var_ratios = svd.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run function\n",
    "select_n_components(svd_var_ratios, 0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run function\n",
    "select_n_components(svd_var_ratios, 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LSI method,3 original terms for the collection are those individ- ual words whose term frequency in the document is more than two. The original term weight of an individual word in a document is set as the corresponding term frequency of that individual word in that document. Then, SVD is used to decompose the original term-docu- ment matrix. Next, we retain a certain percentage of singular values in R of Eq. (8) to produce the approximation matrix, which has lower dimensions than the original term-document matrix. \n",
    "\n",
    "In the next section, we review Latent Semantic Indexing (LSI) (2; 1), which uses the truncated singular value decomposition (SVD) as a low-rank approximation of A. Although the truncated SVD provides the closest approximation to A in Frobenius or L2 norm, LSI ignores the cluster structure while reducing the dimension of the data\n",
    "\n",
    "LSI is based on the assumption that there is some underlying latent semantic structure in the term- document matrix that is corrupted by the wide variety of words used in documents and queries. This is referred to as the problem of polysemy and synonymy (6). The basic idea is that if two document vectors represent the same topic, they will share many associating words with a keyword, and they will have very close semantic structures after dimension reduction via SVD. Thus LSI/SVD breaks the original relationship of the data into linearly independent components (6), where the original term vectors are represented by left singular vectors and document vectors by right singular vectors. That is, if l ≤ rank(A), then\n",
    "\n",
    "The most representative feature extraction algorithm is the Latent Semantic Indexing (LSI) which is an automatic method that transforms the original textual data to a smaller semantic space by taking advantage of some of the implicit higher-order structure in associations of words with text objects [1,2]. The transformation is computed by applying truncated singular value decomposition (SVD) to the term-by-document matrix. After SVD, terms which are used in similar contexts will be merged together. Thus, documents using different terminology to talk about the same concept can be positioned near each other in the new space [17]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
